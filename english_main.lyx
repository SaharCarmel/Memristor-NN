#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Project A: Neural network simulation over Y–Flash memristor devices
\end_layout

\begin_layout Author
Authors: Sahar Carmel 
\begin_inset Formula $305554453$
\end_inset

 ̧Amir Saad 
\begin_inset Formula $305393464$
\end_inset


\begin_inset Newline newline
\end_inset

Superviser: Loai Danial
\end_layout

\begin_layout Abstract
In this project we present a python module based on pytorch ML module that
 presents a neural network architecture based on Y-Flash memristive device
 physical features.
 The comparison we present is based on the differences between state of
 the art neural networks over traditional benchmarks databases vs memristor
 based neural networks.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Background and motivation
\end_layout

\begin_layout Standard
Artificial neural networks (ANN) became a common solution for a wide variety
 of problems in many fields, such as control and pattern recognition.
 Many ANN solutions reached a hardware implementation phase, either commercial
 or with prototypes, aiming to accelerate its performance.
 Recent work has shown that hardware implementation, utilizing nanoscale
 devices, may increase the network performance dramatically, leaving far
 behind their digital and biological counterparts, and approaching the energy
 efficiency of the human brain.
 The background of these advantages is the fact that in analog circuits,
 the vector-matrix multiplication, the key operation of any neuromorphic
 network, is implemented on the physical level.
 The key component of such mixed-signal neuromorphic networks is a device
 with tunable conductance, essentially an analog nonvolatile memory cell,
 mimicking the biological synapse.
 There have been significant recent advances in the development of alternative
 nanoscale nonvolatile memory devices, such as phase change, ferroelectric,
 and magnetic memories.
 In particular, these emerging devices have already been used to demonstrate
 small neuromorphic networks.
 However, their fabrication technology is still in much need for improvement
 and not ready yet for the large-scale integration, which is necessary for
 practically valuable neuromorphic networks.
 This project investigates a network prototype based on mature technology
 of nonvolatile floating-gate memory cells.
\end_layout

\begin_layout Section
Goals and project requirements
\end_layout

\begin_layout Enumerate
Simulating the non-volatile memristive device with virtousuo.
\end_layout

\begin_layout Enumerate
Dicing and Packaging at Towerjazz facilities.
\end_layout

\begin_layout Enumerate
Simulating state of the art neural networks on MNIST data base for comparison.
\end_layout

\begin_layout Enumerate
Using the last network now using Manhattan rule weights update algorithm
 and comparing to the state of the art architecture.
\end_layout

\begin_layout Enumerate
Using the last network now dividing the weights of the network to positive
 and negative weights.
 Comparing the results to the SOTA performance.
\end_layout

\begin_layout Enumerate
Using the last network now limiting weights to physical constraints .
 Comparing the results to the SOTA performance.
\end_layout

\begin_layout Enumerate
Using the last network now using only program operations without clear.
 Comparing the results to the SOTA performance.
\end_layout

\begin_layout Enumerate
Simulating a CNN network using the FF layers with the physical features
 developed above.
\end_layout

\begin_layout Section
Alternative solutions
\end_layout

\begin_layout Section
Algorithm description 
\end_layout

\begin_layout Section
Software implementation
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Section
Problems and solutions
\end_layout

\begin_layout Section
Summary and Conclusions
\end_layout

\begin_layout Section
Appendix
\end_layout

\begin_layout Section
Bibliography 
\end_layout

\end_body
\end_document
