#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Project A: Neural network simulation over Y–Flash memristor devices
\end_layout

\begin_layout Author
Authors: Sahar Carmel 
\begin_inset Formula $305554453$
\end_inset

 ̧Amir Saad 
\begin_inset Formula $305393464$
\end_inset


\begin_inset Newline newline
\end_inset

Superviser: Loai Danial
\end_layout

\begin_layout Abstract
In this project we present a python module based on pytorch ML module that
 presents a neural network architecture based on Y-Flash memristive device
 physical features.
 The comparison we present is based on the differences between state of
 the art neural networks over traditional benchmarks databases vs memristor
 based neural networks.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Background and motivation
\end_layout

\begin_layout Standard
Artificial neural networks (ANN) became a common solution for a wide variety
 of problems in many fields, such as control and pattern recognition.
 Many ANN solutions reached a hardware implementation phase, either commercial
 or with prototypes, aiming to accelerate its performance.
 Recent work has shown that hardware implementation, utilizing nanoscale
 devices, may increase the network performance dramatically, leaving far
 behind their digital and biological counterparts, and approaching the energy
 efficiency of the human brain.
 The background of these advantages is the fact that in analog circuits,
 the vector-matrix multiplication, the key operation of any neuromorphic
 network, is implemented on the physical level.
 The key component of such mixed-signal neuromorphic networks is a device
 with tunable conductance, essentially an analog nonvolatile memory cell,
 mimicking the biological synapse.
 There have been significant recent advances in the development of alternative
 nanoscale nonvolatile memory devices, such as phase change, ferroelectric,
 and magnetic memories.
 In particular, these emerging devices have already been used to demonstrate
 small neuromorphic networks.
 However, their fabrication technology is still in much need for improvement
 and not ready yet for the large-scale integration, which is necessary for
 practically valuable neuromorphic networks.
 This project investigates a network prototype based on mature technology
 of nonvolatile floating-gate memory cells.
\end_layout

\begin_layout Section
Goals and project requirements
\end_layout

\begin_layout Enumerate
Simulating the non-volatile memristive device with virtousuo.
\end_layout

\begin_layout Enumerate
Dicing and Packaging at Towerjazz facilities.
\end_layout

\begin_layout Enumerate
Simulating state of the art neural networks on MNIST data base for comparison.
\end_layout

\begin_layout Enumerate
Using the last network now using Manhattan rule weights update algorithm
 and comparing to the state of the art architecture.
\end_layout

\begin_layout Enumerate
Using the last network now dividing the weights of the network to positive
 and negative weights.
 Comparing the results to the SOTA performance.
\end_layout

\begin_layout Enumerate
Using the last network now limiting weights to physical constraints .
 Comparing the results to the SOTA performance.
\end_layout

\begin_layout Enumerate
Using the last network now using only program operations without clear.
 Comparing the results to the SOTA performance.
\end_layout

\begin_layout Enumerate
Simulating a CNN network using the FF layers with the physical features
 developed above.
\end_layout

\begin_layout Section
Alternative solutions
\end_layout

\begin_layout Standard
Today there are many solutions to engineering problems involving neural
 networks.
 Most networks are programmatically implemented, which means that artificial
 networks are made on non-dedicated hardware.
 Many companies in the market, including Google, Nvidia and Intel, realize
 that there is a limit to the performance of artificial networks and are
 looking for dedicated solutions.
 In recent years, Nvidia and Google have been developing and marketing dedicated
 hardware components for learning and implementing networks by creating
 optimal components for these problems.
 Nevertheless, these hardware components are still limited in performance
 since the transfer of information between the various components, and their
 memory storage is still artificially implemented.
 Therefore, a solution as we propose: a purely analog neuronal network is
 expected to provide significantly higher capabilities than artificial networks.
\end_layout

\begin_layout Section
Algorithm description 
\end_layout

\begin_layout Section
Software implementation
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Section
Problems and solutions
\end_layout

\begin_layout Standard
During our work, we encountered complex difficulties, due in part to the
 fact that the project involves physical characteristics along with programming
 characteristics.
 The main challenge we are faced with is simulating as close as possible
 to the reality of the Memristor device described above and casting its
 physical properties into the neural networks.
 On this issue, we were required to spend most of our time because the open
 source community hardly touche these areas and the work done on the simulation
 of physical devices is inaccessible to use.
 Also, because we used neural network libraries (pytorch) that are accustomed
 to a standard network structure, the integration of changes into the basic
 components of the network was an obstacle to the task accomplishment.
 Among other things, we needed to modify and adapt basic elements such as
 neurons, synapses, activation functions, and behavior in order to accomplish
 true simulation.
\begin_inset Newline newline
\end_inset

Another difficulty rised when we started training the network on databases.
 Because the network we were asked to train was composed of physical components
 in the form of meristors, we were required to work on device sets with
 a limited amount of components.
 As we approached the problem with a low-value array, we found that the
 basket of problems that could be solved given that array was diluted and
 no significant conclusions could be drawn about the use of memory-based
 networks.
 Also, unlike neural networks where each neuron can be accessed individually,
 we had to deal with a memory system programming and deletion, which means
 that we can erase on rows only and not individually.
 Also, a problem that arises from working with memory components is the
 volatility of the information stored therein, naturally because the information
 stored in the memory components is based on the principle of an electron
 floating gate and is not hermetically stored and thus the memory component
 voltage.
 
\begin_inset Newline newline
\end_inset

Regarding the physical nature of the device, as opposed to neuronal networks
 in which the synapse weight value is changed by changing the numeric value
 of the memory stored weight, the physical component programming is required
 to do voltage pulses on the device.
 This creates a situation in which the backpropagation and weighting stage
 requires additional algorithms to improve network performance.
 
\end_layout

\begin_layout Section
Summary and Conclusions
\end_layout

\begin_layout Standard
To summrize, we have shown that using flash-based memory devices to fabricate
 and train neural networks is feasible, when adding the physical restrictions
 we found that network performance decreases when tested against networks
 without memory devices.
 For the complete simulation in which we developed a layer of meristor and
 quantization of the input image, we obtained that the convergence time
 of the grid increased 5-fold relative to networks without the properties
 of the meristor.
 We should note that the optimization of the networks in the Python certainly
 does not take into account the fact that for the same network we received
 a double number of parameters, Since our net weights are represented by
 positive and negative weights as explained earlier.
 In addition, we estimate that the increase in training time is apparently
 a programming problem since in the final realization of the system we intend
 to use physical devices and not the programmatic realization of the devices.
 Also, if we want to improve network performance, we can further optimize
 the network's software implementation by taking advantage of the fact that
 the positive and negative weights of the network can be represented as
 two separate networks and by parallel optimization of the network implementatio
n to improve its performance.
 
\begin_inset Newline newline
\end_inset

Another thing to emphasize is the low network's ability to determine accuracy
 over time.
 As you can see in the graph- [fig: compare-success-percentages] The accuracy
 of the net is having difficult to converge because, unlike the classical
 net where the step size is getting smaller with each epoch, the memristor
 net is getting small step size every epoch, but due to the use of the Manhattan
 rule the gradient step size is constant.
 We assume that if we are able to physically reduce the size of the pulse
 we can have a better convergence of the network.
 Alternatively, one can start the network training with a sequence of several
 pulses and decrease the number of pulses in the same way that the lr decreases.
 
\end_layout

\begin_layout Section
Appendix
\end_layout

\begin_layout Section
Bibliography 
\end_layout

\end_body
\end_document
