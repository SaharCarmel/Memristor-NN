#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Project A: Neural network simulation over Y–Flash memristor devices
\end_layout

\begin_layout Author
Authors: Sahar Carmel 
\begin_inset Formula $305554453$
\end_inset

 ̧Amir Saad 
\begin_inset Formula $305393464$
\end_inset


\begin_inset Newline newline
\end_inset

Superviser: Loai Danial
\end_layout

\begin_layout Abstract
In this project we present a python module based on pytorch ML module that
 presents a neural network architecture based on Y-Flash memristive device
 physical features.
 The comparison we present is based on the differences between state of
 the art neural networks over traditional benchmarks databases vs memristor
 based neural networks.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FloatList figure

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Background and motivation
\end_layout

\begin_layout Standard
Artificial neural networks (ANN) became a common solution for a wide variety
 of problems in many fields, such as control and pattern recognition.
 Many ANN solutions reached a hardware implementation phase, either commercial
 or with prototypes, aiming to accelerate its performance.
 Recent work has shown that hardware implementation, utilizing nanoscale
 devices, may increase the network performance dramatically, leaving far
 behind their digital and biological counterparts, and approaching the energy
 efficiency of the human brain.
 The background of these advantages is the fact that in analog circuits,
 the vector-matrix multiplication, the key operation of any neuromorphic
 network, is implemented on the physical level.
 The key component of such mixed-signal neuromorphic networks is a device
 with tunable conductance, essentially an analog nonvolatile memory cell,
 mimicking the biological synapse.
 There have been significant recent advances in the development of alternative
 nanoscale nonvolatile memory devices, such as phase change, ferroelectric,
 and magnetic memories.
 In particular, these emerging devices have already been used to demonstrate
 small neuromorphic networks.
 However, their fabrication technology is still in much need for improvement
 and not ready yet for the large-scale integration, which is necessary for
 practically valuable neuromorphic networks.
 This project investigates a network prototype based on mature technology
 of nonvolatile floating-gate memory cells.
\end_layout

\begin_layout Section
Goals and project requirements
\end_layout

\begin_layout Enumerate
Simulating the non-volatile memristive device with virtousuo.
\end_layout

\begin_layout Enumerate
Dicing and Packaging at Towerjazz facilities.
\end_layout

\begin_layout Enumerate
Simulating state of the art neural networks on MNIST data base for comparison.
\end_layout

\begin_layout Enumerate
Using the last network now using Manhattan rule weights update algorithm
 and comparing to the state of the art architecture.
\end_layout

\begin_layout Enumerate
Using the last network now dividing the weights of the network to positive
 and negative weights.
 Comparing the results to the SOTA performance.
\end_layout

\begin_layout Enumerate
Using the last network now limiting weights to physical constraints .
 Comparing the results to the SOTA performance.
\end_layout

\begin_layout Enumerate
Using the last network now using only program operations without clear.
 Comparing the results to the SOTA performance.
\end_layout

\begin_layout Enumerate
Simulating a CNN network using the FF layers with the physical features
 developed above.
\end_layout

\begin_layout Section
Alternative solutions
\end_layout

\begin_layout Standard
Today there are many solutions to engineering problems involving neural
 networks.
 Most networks are programmatically implemented, which means that artificial
 networks are made on non-dedicated hardware.
 Many companies in the market, including Google, Nvidia and Intel, realize
 that there is a limit to the performance of artificial networks and are
 looking for dedicated solutions.
 In recent years, Nvidia and Google have been developing and marketing dedicated
 hardware components for learning and implementing networks by creating
 optimal components for these problems.
 Nevertheless, these hardware components are still limited in performance
 since the transfer of information between the various components, and their
 memory storage is still artificially implemented.
 Therefore, a solution as we propose: a purely analog neuronal network is
 expected to provide significantly higher capabilities than artificial networks.
\end_layout

\begin_layout Section
Algorithm description
\end_layout

\begin_layout Subsection
Y-Flash Memristor
\end_layout

\begin_layout Standard
The Y-Flash memristor device is comprised of two NMOS transistors with a
 common Floating gate (FG).
 The control of the FG is via the capacitance between the FG to the common
 drain.
 As we can see in Fig-
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Y-Flash-NVM-device"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Graphics/common gate.PNG
	lyxscale 10
	width 90col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Y-F
\begin_inset CommandInset label
LatexCommand label
name "fig:Y-Flash-NVM-device"

\end_inset

lash NVM device
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
When a positive voltage is applied on the drain, part of it is applied on
 the FG due to Miller capacitance.
 If the voltage goes above 
\begin_inset Formula $V_{th}$
\end_inset

 the cell starts to saturate.
 When higher voltage is applied on the drain, hot electrons are created
 at the drain junction, part of those are injected into the FG which results
 in raise of the threshold voltage.
 The mentioned process is the programming process and this kind of action
 changes the characteristics of the memristor.
 In order to erase the programming, high voltage is applied on the source
 while the drain and the bulk are connected to ground, with this setup there
 are no current inside the device.
 Hot holes are created in the junction, the hot holes are injected into
 the FG which lowers the threshold voltage of the device.
 The program and erase sequence demands a numerous iterations in order to
 achieve desired threshold voltages as we can see in Fig-
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:-vs-program"
plural "false"
caps "false"
noprefix "false"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Graphics/prog-erase iterartion.PNG
	lyxscale 10
	width 90col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset Formula $V_{th}$
\end_inset

 vs program iterations
\begin_inset CommandInset label
LatexCommand label
name "fig:-vs-program"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As we can see in order to achieve certain resistance numerous iterations
 needed to be applied.
 
\end_layout

\begin_layout Subsection
Manhattan Rule
\end_layout

\begin_layout Standard
Neuron networks consist of layered perceptron networks as seen in Fig-
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Analog-neural-network"
plural "false"
caps "false"
noprefix "false"

\end_inset

 a.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Graphics/neural_netowrk_manhattan.png
	lyxscale 10
	width 90col%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Analog 
\begin_inset CommandInset label
LatexCommand label
name "fig:Analog-neural-network"

\end_inset

neural network architecture
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As can be seen, each neuron is represented by the sum of synapses attached
 to it and transmits the sum through a nonlinear function defined when used.
 The transition to analog use in neuronal servers is depicted in the Fig-
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Analog-neural-network"
plural "false"
caps "false"
noprefix "false"

\end_inset

 b.
 Each node indicates a synapse and weights are represented by resistors
 and outputs sum the currents into an op-amp followed by a non-linear function.
 Because the weights of synapses can be negative, the analog network size
 is multiplied by the amount of synapses since there is one that represents
 a positive weight and one that represents a negative weight.
 And so the weight of each synapse is defined by 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
W_{ij}=G_{ij}^{+}-G_{ij}^{-}
\]

\end_inset

 And like so we can create artificially negative weights.
 Fig- 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Analog-neural-network"
plural "false"
caps "false"
noprefix "false"

\end_inset

c shows the complete analog neuron network architecture where the voltages
 V are the network inputs and the op-amp outputs the network classification.
\begin_inset Newline newline
\end_inset

As part of the network learning process, the weight of synapses are modified
 in order to improve the network's performance.
 In order to create a convenient weight change platform, we will use Meristors
 on Y-Flash NVM Memristor technology.
 Because many changes are required in each learning iteration of the network
 and each component is individually required to change the resistor resistance,
 this rule proposes to change the resistors in only two directions in one
 intensity at a time.
 
\begin_inset Formula 
\[
\Delta W_{ij}^{M}=sgn\left[\Delta W_{ij}\right]
\]

\end_inset

 In other words, the change in the weight of the synapse is related only
 to the gradient sign.
 We will investigate the impact of this rule on the process of training.
\end_layout

\begin_layout Subsection
Y-Flash Memristor array
\end_layout

\begin_layout Section
Software implementation
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Section
Problems and solutions
\end_layout

\begin_layout Standard
During our work, we encountered complex difficulties, due in part to the
 fact that the project involves physical characteristics along with programming
 characteristics.
 The main challenge we are faced with is simulating as close as possible
 to the reality of the Memristor device described above and casting its
 physical properties into the neural networks.
 On this issue, we were required to spend most of our time because the open
 source community hardly touche these areas and the work done on the simulation
 of physical devices is inaccessible to use.
 Also, because we used neural network libraries (pytorch) that are accustomed
 to a standard network structure, the integration of changes into the basic
 components of the network was an obstacle to the task accomplishment.
 Among other things, we needed to modify and adapt basic elements such as
 neurons, synapses, activation functions, and behavior in order to accomplish
 true simulation.
\begin_inset Newline newline
\end_inset

Another difficulty rised when we started training the network on databases.
 Because the network we were asked to train was composed of physical components
 in the form of meristors, we were required to work on device sets with
 a limited amount of components.
 As we approached the problem with a low-value array, we found that the
 basket of problems that could be solved given that array was diluted and
 no significant conclusions could be drawn about the use of memory-based
 networks.
 Also, unlike neural networks where each neuron can be accessed individually,
 we had to deal with a memory system programming and deletion, which means
 that we can erase on rows only and not individually.
 Also, a problem that arises from working with memory components is the
 volatility of the information stored therein, naturally because the information
 stored in the memory components is based on the principle of an electron
 floating gate and is not hermetically stored and thus the memory component
 voltage.
 
\begin_inset Newline newline
\end_inset

Regarding the physical nature of the device, as opposed to neuronal networks
 in which the synapse weight value is changed by changing the numeric value
 of the memory stored weight, the physical component programming is required
 to do voltage pulses on the device.
 This creates a situation in which the backpropagation and weighting stage
 requires additional algorithms to improve network performance.
 
\end_layout

\begin_layout Section
Summary and Conclusions
\end_layout

\begin_layout Standard
To summrize, we have shown that using flash-based memory devices to fabricate
 and train neural networks is feasible, when adding the physical restrictions
 we found that network performance decreases when tested against networks
 without memory devices.
 For the complete simulation in which we developed a layer of meristor and
 quantization of the input image, we obtained that the convergence time
 of the grid increased 5-fold relative to networks without the properties
 of the meristor.
 We should note that the optimization of the networks in the Python certainly
 does not take into account the fact that for the same network we received
 a double number of parameters, Since our net weights are represented by
 positive and negative weights as explained earlier.
 In addition, we estimate that the increase in training time is apparently
 a programming problem since in the final realization of the system we intend
 to use physical devices and not the programmatic realization of the devices.
 Also, if we want to improve network performance, we can further optimize
 the network's software implementation by taking advantage of the fact that
 the positive and negative weights of the network can be represented as
 two separate networks and by parallel optimization of the network implementatio
n to improve its performance.
 
\begin_inset Newline newline
\end_inset

Another thing to emphasize is the low network's ability to determine accuracy
 over time.
 As you can see in the graph- [fig: compare-success-percentages] The accuracy
 of the net is having difficult to converge because, unlike the classical
 net where the step size is getting smaller with each epoch, the memristor
 net is getting small step size every epoch, but due to the use of the Manhattan
 rule the gradient step size is constant.
 We assume that if we are able to physically reduce the size of the pulse
 we can have a better convergence of the network.
 Alternatively, one can start the network training with a sequence of several
 pulses and decrease the number of pulses in the same way that the lr decreases.
 
\end_layout

\begin_layout Section
Appendix
\end_layout

\begin_layout Section
Bibliography 
\end_layout

\end_body
\end_document
